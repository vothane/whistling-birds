{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "whistling-birds.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vothane/whistling-birds/blob/master/whistling_birds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8001QwH2nejQ",
        "colab_type": "text"
      },
      "source": [
        "# Replacing back-propogation with PSO (Particle Swarm Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRyMDUcGnmTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "class ParticleSwarmOptimizedNN():\n",
        "  def __init__(self, population_size, model_builder, inertia_weight=0.8, cognitive_weight=2, social_weight=2, max_velocity=20):\n",
        "    self.population_size = population_size\n",
        "    self.model_builder = model_builder\n",
        "    self.best_individual = None\n",
        "    # Parameters used to update velocity\n",
        "    self.cognitive_w = cognitive_weight\n",
        "    self.inertia_w = inertia_weight\n",
        "    self.social_w = social_weight\n",
        "    self.min_v = -max_velocity\n",
        "    self.max_v = max_velocity\n",
        "\n",
        "  def _build_model(self, id):\n",
        "    \"\"\" Returns a new particle\"\"\"\n",
        "    class ParticleSwarmOptimizer: \n",
        "      pass\n",
        "    pso = ParticleSwarmOptimizer() \n",
        "    pso.model = self.model_builder(n_inputs=self.X.shape[1], n_outputs=self.y.shape[1])\n",
        "    pso.id = id\n",
        "    pso.fitness = 0\n",
        "    pso.highest_fitness = 0\n",
        "    pso.accuracy = 0\n",
        "    # Set intial best as the current initialization\n",
        "    pso.best_layers = copy.copy(pso.model.layers)\n",
        "\n",
        "    # Set initial velocity to zero\n",
        "    pso.velocity = []\n",
        "    for layer in pso.model.layers:\n",
        "      velocity = {\"W\": 0, \"w0\": 0}\n",
        "      weights = layer.get_weights()[0]\n",
        "      biases = layer.get_weights()[1]\n",
        "      velocity = {\"W\": np.zeros_like(weights), \"w0\": np.zeros_like(biases)}\n",
        "      pso.velocity.append(velocity)\n",
        "\n",
        "    return pso\n",
        "\n",
        "  def _initialize_population(self):\n",
        "    \"\"\" Initialization of the neural networks forming the population\"\"\"\n",
        "    self.population = []\n",
        "    for i in range(self.population_size):\n",
        "      model = self._build_model(id=i)\n",
        "      self.population.append(model)\n",
        " \n",
        "  def _update_weights(self, individual):\n",
        "    \"\"\" Calculate the new velocity and update weights for each layer \"\"\"\n",
        "    # Two random parameters used to update the velocity\n",
        "    r1 = np.random.uniform()\n",
        "    r2 = np.random.uniform()\n",
        "\n",
        "    for i, layer in enumerate(individual.model.layers):\n",
        "      # Layer weights velocity\n",
        "      weights = layer.get_weights()[0]\n",
        "      biases = layer.get_weights()[1]\n",
        "             \n",
        "      first_term_W = self.inertia_w * individual.velocity[i][\"W\"]\n",
        "      second_term_W = self.cognitive_w * r1 * (individual.best_layers[i].get_weights()[0] - weights)\n",
        "      third_term_W = self.social_w * r2 * (self.best_individual.model.layers[i].get_weights()[0] - weights)\n",
        "      new_velocity = first_term_W + second_term_W + third_term_W\n",
        "      individual.velocity[i][\"W\"] = np.clip(new_velocity, self.min_v, self.max_v)\n",
        "\n",
        "      # Bias weight velocity\n",
        "      first_term_w0 = self.inertia_w * individual.velocity[i][\"w0\"]\n",
        "      second_term_w0 = self.cognitive_w * r1 * (individual.best_layers[i].get_weights()[1] - biases)\n",
        "      third_term_w0 = self.social_w * r2 * (self.best_individual.model.layers[i].get_weights()[1] - biases)\n",
        "      new_velocity = first_term_w0 + second_term_w0 + third_term_w0\n",
        "      individual.velocity[i][\"w0\"] = np.clip(new_velocity, self.min_v, self.max_v)\n",
        "\n",
        "      # Update layer weights with velocity\n",
        "      weights += individual.velocity[i][\"W\"]\n",
        "      K.set_value(layer.weights[0], weights)\n",
        "      biases += individual.velocity[i][\"w0\"]\n",
        "      K.set_value(layer.weights[1], biases)\n",
        "        \n",
        "  def _calculate_fitness(self, individual):\n",
        "    \"\"\" Evaluate the individual on the test set to get fitness scores \"\"\"\n",
        "    loss, acc = individual.model.test_on_batch(self.X, self.y)\n",
        "    individual.fitness = 1 / (loss + 1e-8)\n",
        "    individual.accuracy = acc\n",
        "\n",
        "  def optimize(self, X, y, n_generations):\n",
        "    \"\"\" Will evolve the population for n_generations based on dataset X and labels y\"\"\"\n",
        "    self.X, self.y = X, y\n",
        "    self._initialize_population()\n",
        "\n",
        "    # The best individual of the population is initialized as population's first ind.\n",
        "    self.best_individual = copy.copy(self.population[0])\n",
        "\n",
        "    for epoch in range(n_generations):\n",
        "      for individual in self.population:\n",
        "        # Calculate new velocity and update the NN weights\n",
        "        self._update_weights(individual)\n",
        "        # Calculate the fitness of the updated individual\n",
        "        self._calculate_fitness(individual)\n",
        "\n",
        "        # If the current fitness is higher than the individual's previous highest\n",
        "        # => update the individual's best layer setup\n",
        "        if individual.fitness > individual.highest_fitness:\n",
        "          individual.best_layers = copy.copy(individual.model.layers)\n",
        "          individual.highest_fitness = individual.fitness\n",
        "\n",
        "        # If the individual's fitness is higher than the highest recorded fitness for the\n",
        "        # whole population => update the best individual\n",
        "        if individual.fitness > self.best_individual.fitness:\n",
        "          self.best_individual = copy.copy(individual)\n",
        "\n",
        "      print (\"[%d Best Individual - ID: %d Fitness: %.5f, Accuracy: %.1f%%]\" % \n",
        "             (epoch,\n",
        "              self.best_individual.id,\n",
        "              self.best_individual.fitness,\n",
        "              100*float(self.best_individual.accuracy)))\n",
        "      \n",
        "    return self.best_individual"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ful_VBlzoG2W",
        "colab_type": "code",
        "outputId": "48123bf4-061f-45a9-9202-6f6b368a607e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def main():\n",
        "  iris = datasets.load_iris()\n",
        "  X = iris['data']\n",
        "  y = iris['target']\n",
        "  names = iris['target_names']\n",
        "  feature_names = iris['feature_names']\n",
        "\n",
        "  # One hot encoding\n",
        "  enc = preprocessing.OneHotEncoder()\n",
        "  Y = enc.fit_transform(y[:, np.newaxis]).toarray()\n",
        "\n",
        "  # Scale data to have mean 0 and variance 1 \n",
        "  # which is importance for convergence of the neural network\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.5, random_state=2)\n",
        "\n",
        "  n_features = X.shape[1]\n",
        "  n_classes = Y.shape[1]\n",
        "\n",
        "  # Model builder\n",
        "  def model_builder(n_inputs, n_outputs): \n",
        "    model = Sequential()\n",
        "    model.add(Dense(10, input_shape=(4,), activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Adam optimizer with learning rate of 0.001\n",
        "    optimizer = Adam(lr=0.001)\n",
        "    model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  # Print the model summary of a individual in the population\n",
        "  print (\"\")\n",
        "  model_builder(n_inputs=n_features, n_outputs=n_classes).summary()\n",
        "\n",
        "  population_size = 100\n",
        "  n_generations = 10\n",
        "\n",
        "  inertia_weight = 0.8\n",
        "  cognitive_weight = 0.8\n",
        "  social_weight = 0.8\n",
        "\n",
        "  print (\"Population Size: %d\" % population_size)\n",
        "  print (\"Generations: %d\" % n_generations)\n",
        "  print (\"\")\n",
        "  print (\"Inertia Weight: %.2f\" % inertia_weight)\n",
        "  print (\"Cognitive Weight: %.2f\" % cognitive_weight)\n",
        "  print (\"Social Weight: %.2f\" % social_weight)\n",
        "  print (\"\")\n",
        "\n",
        "  pso = ParticleSwarmOptimizedNN(\n",
        "          population_size=population_size, \n",
        "          inertia_weight=inertia_weight,\n",
        "          cognitive_weight=cognitive_weight,\n",
        "          social_weight=social_weight,\n",
        "          max_velocity=5,\n",
        "          model_builder=model_builder)\n",
        "    \n",
        "  pso = pso.optimize(X_train, y_train, n_generations=n_generations)\n",
        "\n",
        "  loss, accuracy = pso.model.test_on_batch(X_test, y_test)\n",
        "\n",
        "  print (\"Accuracy: %.1f%%\" % float(100*accuracy))\n",
        "\n",
        "  # Reduce dimension to 2D using PCA and plot the results\n",
        "  y_pred = np.argmax(pso.model.predict(X_test), axis=1)\n",
        "  #Plot().plot_in_2d(X_test, y_pred, title=\"Particle Swarm Optimized Neural Network\", accuracy=accuracy, legend_labels=range(y.shape[1]))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model: \"sequential_312\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_934 (Dense)            (None, 10)                50        \n",
            "_________________________________________________________________\n",
            "dense_935 (Dense)            (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_936 (Dense)            (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 193\n",
            "Trainable params: 193\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Population Size: 100\n",
            "Generations: 10\n",
            "\n",
            "Inertia Weight: 0.80\n",
            "Cognitive Weight: 0.80\n",
            "Social Weight: 0.80\n",
            "\n",
            "[0 Best Individual - ID: 99 Fitness: 1.03064, Accuracy: 58.7%]\n",
            "[1 Best Individual - ID: 91 Fitness: 1.19143, Accuracy: 53.3%]\n",
            "[2 Best Individual - ID: 76 Fitness: 1.25379, Accuracy: 62.7%]\n",
            "[3 Best Individual - ID: 3 Fitness: 1.50093, Accuracy: 62.7%]\n",
            "[4 Best Individual - ID: 62 Fitness: 1.71079, Accuracy: 64.0%]\n",
            "[5 Best Individual - ID: 69 Fitness: 2.22350, Accuracy: 74.7%]\n",
            "[6 Best Individual - ID: 78 Fitness: 2.97693, Accuracy: 82.7%]\n",
            "[7 Best Individual - ID: 21 Fitness: 3.43931, Accuracy: 80.0%]\n",
            "[8 Best Individual - ID: 21 Fitness: 3.43931, Accuracy: 80.0%]\n",
            "[9 Best Individual - ID: 21 Fitness: 3.43931, Accuracy: 80.0%]\n",
            "Accuracy: 69.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16mB_ICy7Co_",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "The best individual in generation 6 got an accuracy score of __82%__ which is pretty good. The whole process takes a while since we are using just the CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYvTbJxqxB-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66e92963-64cd-4469-a05b-d82ffb3a0abc"
      },
      "source": [
        "import random\n",
        "import functools\n",
        "\n",
        "def search(epochs, search_space, num_bees, num_sites, best_sites, patch_size, elites, workers):\n",
        "    best = None\n",
        "    population = []\n",
        "    for _ in range(num_bees):\n",
        "        population.append(create_random_bee(search_space))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for bee in population:\n",
        "            bee.update({'fitness': objective_fn(bee['vector'])})\n",
        "        population.sort(key=lambda bee: bee['fitness'])\n",
        "        candidate = population[0]\n",
        "        if best is None or candidate['fitness'] < best['fitness']:\n",
        "            best = population[0]\n",
        "\n",
        "        next_gen = []\n",
        "        for i, parent in enumerate(population[:num_sites]):\n",
        "            neighbor_size = elites if i < best_sites else workers\n",
        "            next_gen.append(search_neighbor(parent, neighbor_size, patch_size, search_space))\n",
        "\n",
        "        scouts = create_scout_bees(search_space, (num_bees - num_sites))\n",
        "        population = next_gen + scouts\n",
        "        patch_size = patch_size * 0.98\n",
        "\n",
        "    return best\n",
        "\n",
        "\n",
        "def objective_fn(X):\n",
        "    objective_value = functools.reduce(lambda sum, x: sum + (x ** 2.0), X)\n",
        "    return objective_value\n",
        "\n",
        "def create_random_bee(search_space):\n",
        "    return {'vector': random_vector(search_space)}\n",
        "\n",
        "def random_vector(minmax):\n",
        "    return [minmax[i][0] + ((minmax[i][1] - minmax[i][0]) * random.random()) for i in range(len(minmax))]\n",
        "\n",
        "def create_scout_bees(search_space, num_scouts):\n",
        "    return [create_random_bee(search_space) for i in range(num_scouts)]\n",
        "\n",
        "def search_neighbor(parent, neighbor_size, patch_size, search_space):\n",
        "    neighbors = []\n",
        "\n",
        "    for _ in range(neighbor_size):\n",
        "        neighbors.append(create_neighbor_bee(parent['vector'], patch_size, search_space))\n",
        "\n",
        "    for bee in neighbors:\n",
        "        bee.update({'fitness': objective_fn(bee['vector'])})\n",
        "\n",
        "    neighbors.sort(key=lambda bee: bee['fitness'])\n",
        "    return neighbors.pop()\n",
        "\n",
        "def create_neighbor_bee(space, patch_size, search_space):\n",
        "    vectors = []\n",
        "    for i, vec in enumerate(space):\n",
        "        vec = (vec + random.random() * patch_size) if random.random() < 0.5 else (vec - random.random() * patch_size)\n",
        "        vec = search_space[i][0] if vec < search_space[i][0] else search_space[i][1]\n",
        "        vectors.append(vec)\n",
        "\n",
        "    bee = {}\n",
        "    bee['vector'] = vectors\n",
        "    return bee\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    problem_size = 3\n",
        "    search_space = [[0, 1] for _ in range(problem_size)]\n",
        "\n",
        "    epochs = 500\n",
        "    num_bees = 45\n",
        "    num_sites = 3\n",
        "    best_sites = 1\n",
        "    patch_size = 3.0\n",
        "    elites = 7\n",
        "    workers = 2\n",
        "    best_bee = search(epochs, search_space, num_bees, num_sites, best_sites, patch_size, elites, workers)\n",
        "    print(best_bee)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'vector': [0, 0, 0], 'fitness': 0.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGd0MgkwxP0p",
        "colab_type": "text"
      },
      "source": [
        "The best bee should always be with a fitness value of 0 since we optiminizing (minimizing) the function *sum + (x ** 2.0)*. Later we will train weights on a artificial neural network as we did with PSO.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM7T0_oAyG5M",
        "colab_type": "text"
      },
      "source": [
        ">.    \n",
        ">.  \n",
        ">.  \n",
        ">.  \n",
        ">.  \n",
        ">.  \n",
        ">.  \n",
        ">.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmSWErLU7kzw",
        "colab_type": "text"
      },
      "source": [
        "> __them__: *Knock Knock*  \n",
        "> __me__: *\\\"Who's there\\?\"*  \n",
        "> __them__: *\\\"Couple of Ruby dudes checking out your shitty code and negatively criticizing it. Because we're experts on everything. EVERYTHING\\\"*  \n",
        "> __me__: \n",
        "\n",
        "![](https://media.giphy.com/media/j6qnuNv4HoxrlUcjSR/giphy-downsized.gif)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E7f0y7ZANb-",
        "colab_type": "text"
      },
      "source": [
        "# ABC Artificial Bee Colony"
      ]
    }
  ]
}