{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "whistling-birds.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vothane/whistling-birds/blob/master/whistling_birds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8001QwH2nejQ",
        "colab_type": "text"
      },
      "source": [
        "# Replacing back-propogation with PSO (Particle Swarm Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRyMDUcGnmTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "class ParticleSwarmOptimizedNN():\n",
        "  def __init__(self, population_size, model_builder, inertia_weight=0.8, cognitive_weight=2, social_weight=2, max_velocity=20):\n",
        "    self.population_size = population_size\n",
        "    self.model_builder = model_builder\n",
        "    self.best_individual = None\n",
        "    # Parameters used to update velocity\n",
        "    self.cognitive_w = cognitive_weight\n",
        "    self.inertia_w = inertia_weight\n",
        "    self.social_w = social_weight\n",
        "    self.min_v = -max_velocity\n",
        "    self.max_v = max_velocity\n",
        "\n",
        "  def _build_model(self, id):\n",
        "    \"\"\" Returns a new particle\"\"\"\n",
        "    model = self.model_builder(n_inputs=self.X.shape[1], n_outputs=self.y.shape[1])\n",
        "    model.id = id\n",
        "    model.fitness = 0\n",
        "    model.highest_fitness = 0\n",
        "    model.accuracy = 0\n",
        "    # Set intial best as the current initialization\n",
        "    model.best_layers = copy.copy(model.model.layers)\n",
        "\n",
        "    # Set initial velocity to zero\n",
        "    model.velocity = []\n",
        "    for layer in model.model.layers:\n",
        "      velocity = {\"W\": 0, \"w0\": 0}\n",
        "      weights = layer.get_weights()[0]\n",
        "      biases = layer.get_weights()[1]\n",
        "      velocity = {\"W\": np.zeros_like(weights), \"w0\": np.zeros_like(biases)}\n",
        "      model.velocity.append(velocity)\n",
        "\n",
        "    return model\n",
        "\n",
        "  def _initialize_population(self):\n",
        "    \"\"\" Initialization of the neural networks forming the population\"\"\"\n",
        "    self.population = []\n",
        "    for i in range(self.population_size):\n",
        "      model = self._build_model(id=i)\n",
        "      self.population.append(model)\n",
        " \n",
        "  def _update_weights(self, individual):\n",
        "    \"\"\" Calculate the new velocity and update weights for each layer \"\"\"\n",
        "    # Two random parameters used to update the velocity\n",
        "    r1 = np.random.uniform()\n",
        "    r2 = np.random.uniform()\n",
        "\n",
        "    for i, layer in enumerate(individual.model.layers):\n",
        "      # Layer weights velocity\n",
        "      weights = layer.get_weights()[0]\n",
        "      biases = layer.get_weights()[1]\n",
        "             \n",
        "      first_term_W = self.inertia_w * individual.velocity[i][\"W\"]\n",
        "      second_term_W = self.cognitive_w * r1 * (individual.best_layers[i].get_weights()[0] - weights)\n",
        "      third_term_W = self.social_w * r2 * (self.best_individual.layers[i].get_weights()[0] - weights)\n",
        "      new_velocity = first_term_W + second_term_W + third_term_W\n",
        "      individual.velocity[i][\"W\"] = np.clip(new_velocity, self.min_v, self.max_v)\n",
        "\n",
        "      # Bias weight velocity\n",
        "      first_term_w0 = self.inertia_w * individual.velocity[i][\"w0\"]\n",
        "      second_term_w0 = self.cognitive_w * r1 * (individual.best_layers[i].get_weights()[1] - biases)\n",
        "      third_term_w0 = self.social_w * r2 * (self.best_individual.layers[i].get_weights()[1] - biases)\n",
        "      new_velocity = first_term_w0 + second_term_w0 + third_term_w0\n",
        "      individual.velocity[i][\"w0\"] = np.clip(new_velocity, self.min_v, self.max_v)\n",
        "\n",
        "      # Update layer weights with velocity\n",
        "      weights += individual.velocity[i][\"W\"]\n",
        "      K.set_value(layer.weights[0],  weights)\n",
        "      biases += individual.velocity[i][\"w0\"]\n",
        "      K.set_value(layer.weights[1],  biases)\n",
        "        \n",
        "  def _calculate_fitness(self, individual):\n",
        "    \"\"\" Evaluate the individual on the test set to get fitness scores \"\"\"\n",
        "    loss, acc = individual.test_on_batch(self.X, self.y)\n",
        "    individual.fitness = 1 / (loss + 1e-8)\n",
        "    individual.accuracy = acc\n",
        "\n",
        "  def optimize(self, X, y, n_generations):\n",
        "    \"\"\" Will evolve the population for n_generations based on dataset X and labels y\"\"\"\n",
        "    self.X, self.y = X, y\n",
        "    self._initialize_population()\n",
        "\n",
        "    # The best individual of the population is initialized as population's first ind.\n",
        "    self.best_individual = copy.copy(self.population[0])\n",
        "\n",
        "    for epoch in range(n_generations):\n",
        "      for individual in self.population:\n",
        "        # Calculate new velocity and update the NN weights\n",
        "        self._update_weights(individual)\n",
        "        # Calculate the fitness of the updated individual\n",
        "        self._calculate_fitness(individual)\n",
        "\n",
        "        # If the current fitness is higher than the individual's previous highest\n",
        "        # => update the individual's best layer setup\n",
        "        if individual.fitness > individual.highest_fitness:\n",
        "          individual.best_layers = copy.copy(individual.layers)\n",
        "          individual.highest_fitness = individual.fitness\n",
        "\n",
        "        # If the individual's fitness is higher than the highest recorded fitness for the\n",
        "        # whole population => update the best individual\n",
        "        if individual.fitness > self.best_individual.fitness:\n",
        "          self.best_individual = copy.copy(individual)\n",
        "\n",
        "      print (\"[%d Best Individual - ID: %d Fitness: %.5f, Accuracy: %.1f%%]\" % \n",
        "             (epoch,\n",
        "              self.best_individual.id,\n",
        "              self.best_individual.fitness,\n",
        "              100*float(self.best_individual.accuracy)))\n",
        "      \n",
        "    return self.best_individual"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ful_VBlzoG2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def main():\n",
        "  iris = datasets.load_iris()\n",
        "  X = iris['data']\n",
        "  y = iris['target']\n",
        "  names = iris['target_names']\n",
        "  feature_names = iris['feature_names']\n",
        "\n",
        "  # One hot encoding\n",
        "  enc = preprocessing.OneHotEncoder()\n",
        "  Y = enc.fit_transform(y[:, np.newaxis]).toarray()\n",
        "\n",
        "  # Scale data to have mean 0 and variance 1 \n",
        "  # which is importance for convergence of the neural network\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.5, random_state=2)\n",
        "\n",
        "  n_features = X.shape[1]\n",
        "  n_classes = Y.shape[1]\n",
        "\n",
        "  # Model builder\n",
        "  def model_builder(n_inputs, n_outputs): \n",
        "    model = Sequential()\n",
        "    model.add(Dense(10, input_shape=(4,), activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Adam optimizer with learning rate of 0.001\n",
        "    optimizer = Adam(lr=0.001)\n",
        "    model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  # Print the model summary of a individual in the population\n",
        "  print (\"\")\n",
        "  model_builder(n_inputs=n_features, n_outputs=n_classes).summary()\n",
        "\n",
        "  population_size = 100\n",
        "  n_generations = 10\n",
        "\n",
        "  inertia_weight = 0.8\n",
        "  cognitive_weight = 0.8\n",
        "  social_weight = 0.8\n",
        "\n",
        "  print (\"Population Size: %d\" % population_size)\n",
        "  print (\"Generations: %d\" % n_generations)\n",
        "  print (\"\")\n",
        "  print (\"Inertia Weight: %.2f\" % inertia_weight)\n",
        "  print (\"Cognitive Weight: %.2f\" % cognitive_weight)\n",
        "  print (\"Social Weight: %.2f\" % social_weight)\n",
        "  print (\"\")\n",
        "\n",
        "  model = ParticleSwarmOptimizedNN(\n",
        "            population_size=population_size, \n",
        "            inertia_weight=inertia_weight,\n",
        "            cognitive_weight=cognitive_weight,\n",
        "            social_weight=social_weight,\n",
        "            max_velocity=5,\n",
        "            model_builder=model_builder)\n",
        "    \n",
        "  model = model.optimize(X_train, y_train, n_generations=n_generations)\n",
        "\n",
        "  loss, accuracy = model.test_on_batch(X_test, y_test)\n",
        "\n",
        "  print (\"Accuracy: %.1f%%\" % float(100*accuracy))\n",
        "\n",
        "  # Reduce dimension to 2D using PCA and plot the results\n",
        "  y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "  Plot().plot_in_2d(X_test, y_pred, title=\"Particle Swarm Optimized Neural Network\", accuracy=accuracy, legend_labels=range(y.shape[1]))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}